---
output: github_document
number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(lubridate)
library(ggplot2)
library(scico)

savePath <- paste0("~/ClimDesign_PhD/XGBoost-GAM index flood model","/coms/",
                   "markdown/","markdown data/")
load(paste0(savePath,"exampleplots_fjern_manglende_data.rda"))
load(paste0(savePath,"tidsopplxsning_datapack.rda"))
load(paste0(savePath,"station-years.rda"))
```

<h1 align="center">floodGAM datasets</h1>

## Overview
As part of the floodGAM analysis, we developed a flood dataset focused on sub-daily sampling frequency. 

NVE report 2016:85 [Flomdata: utvalg og kvalitetssikring av flomdata for flomfrekvensanalyser](https://asp.bibliotekservice.no/nve/title.aspx?tkey=23147) identifies 529 stations suitable for flood frequency analysis. We independently evaluated each year of data at these stations for ability to capture annual maxima at sub-daily sampling frequency. The result was a set of 250 stations (the `gamfelt` dataset), each with at least 20 years of total data and at least 10 years of sub-daily data.

We provide both (i) the `gamfelt` dataset and (ii) all scripts and resources needed to recreate the dataset from the raw data stored in HYDRA II.

Feedback on the dataset is very welcome. 

## How to get the data

The annual maxima, catchment descriptors, descriptive table and statistical summary of the `gamfelt` dataset are stored in [`/data/processed-data/gamfelt/`](/data/processed-data/gamfelt/)


## File structure

In addition to the `gamfelt` dataset, this data folder contains many data products relevant to the larger floodGAM analysis (e.g. model fitting and evaluation).

 - `processed-data` – Any data loaded/manipulated/changed/saved with code from the `code` folders.  

 - `raw-data` – HYDRA II database commands and other raw data.

 - `how-to-guides` – how to use the database commands in `raw-data`.


## Data pipeline

The data pipeline is the process of building the `gamfelt` dataset from the raw streamflow data provided by NVE. The "findata quality control" step is described in more detail in the following section.

### Dependencies

Building the dataset with the scripts in this repository requires: 

 - the NVE database **HYDRA II**
 - access to the internal NVE system **lescon_var**
 - the programming language **R**.  

Some of the intermediate data files are large (part of the quality control requires downloading and cross-checking the HYKVALP-ICECORR database with HYDAG). Any data file over 50 Mb is stored on zenodo. 

| Action | Description | Requires | Output saved? | Where? |
| --- | --- | --- | :---: | :---: |
| Get streamflow data | Download data from HYDRA II | lescon_var (internal system), [`lescon_var_commands.txt`](/data/raw-data/), [lescon_var user guide](/data/how-to/hvordan_henter_jeg_data_med_lescon_var.md) | - | - |
| Change formatting | Change downloaded data to .rds format  | [`clean-and-process-rawdata-from-database.R`](/code/scripts/data-creation/) | yes | zenodo |
| Findata quality control | Choose excluded years/stations, handle missing data, check minimum time spacing at peaks, enforce minimum record length | [`quality-control-streamflow-data.R`](/code/scripts/data-creation/), [`utelatt.csv`](/data/raw-data/) | yes | zenodo |
| Process data | Select annual maxima | `write-this-script.R` | yes | github |


## Findata quality control

This step is how we check for sub-daily sampling frequency. when possible, the steps should be systematisk (i.e. minimum data in manual exclusion). if we have prior reasons to exclude the manual stations. 

### 1. Manually select excluded years / stations

Report [2016:85](https://asp.bibliotekservice.no/nve/title.aspx?tkey=23147) identifies certain years that should be excluded from flood frequency analysis. In addition we have manually identified some years and stations that should be excluded if we need sub-daily sampling frequency.

 - [`utelatt.csv`](/data/raw-data/) - list of years and stations that should be excluded
 - [`utelatt_notes.xlsx`](/data/raw-data/) - some notes on manually removed years

### 2. Handle missing data and check minimum time spacing at peaks

NVE has no "perfect" archive for either fine data or daily data. Some archives are ice-reduced, while others are not. The same applies to completeness.

We use data from the HYKVALP-ICECORR archive (archive 35), which has primarily controlled data with fine/variable time resolution and is virtually ice-reduced. HYKVALP-ICECORR is not secondarily controlled or complete[^1].

[^1]: Det kommer snart (slutten av 2024/tidlig 2025) en oppdatering i databasen. HYKVAL-data skal bli sekundærkontrollert etter en bestemt dato. Data før denne datoen skal ikke endres.

Because the HYKVAL data is incomplete, we must make decisions on how to handle years with missing data. We choose to cross-check the HYKVALP-ICECORR data with data from another archive, HYDAG (archive 05), which contains controlled, complete daily data that is ice-reduced and re-checked.

The cross-checking has two components:

#### 2.1 Check the minimum number of days per year in HYKVALP-ICECORR and HYDAG

First we count the number of days per year for every year and every station. Then we can remove years that have less than 200 days of observations in HYKVALP-ICECORR or less than 300 days in both HYKVALP-ICECORR and HYDAG. We can look at some of the years that we remove:

##### Example of years removed due to < 200 days in HYKVALP-ICECORR:

```{r,echo=F, fig.dim=c(10,4), warning=F}
ggplot(look200day.hykval[.(these.200day)]) +
  geom_line(aes(date,cumecs,color="hykval"),size=1.1) +
  geom_line(data=look200day.hydag[.(these.200day)],
            aes(date,cumecs,color="hydag")) +
  labs(y = "m3/s") +
  facet_wrap(ID~yk,scales="free", nrow=1) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = "bottom",legend.title = element_blank())
```

##### Example of years removed due to < 300 days in both archives:

```{r,echo=F, fig.dim=c(10,4), warning=F}
ggplot(look300day.hykval[.(these.300day)]) +
  geom_line(aes(perc.yr,cumecs,color="hykval"),size=1.1) +
  geom_line(data=look300day.hydag[.(these.300day)],
            aes(perc.yr,cumecs,color="hydag")) +
  labs(y = "m3/s",x="date as percent of the year") +
  facet_wrap(ID~yk,scales="free_y", nrow=1) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = "bottom",legend.title = element_blank())
```

#### 2.2 Check minimum time spacing using HYDAG annual maxima

The criteria for cross-checking with HYDAG are a bit more complicated. HYDAG and HYKVALP-ICECORR do not perfectly match. There are some stations and years that are in HYKVALP-ICECORR but not in HYDAG, and vice versa.

```{r, eval = FALSE}
# find unique station-year combinations in both hydag and hykvalp-icecorr:
hykval.sy <- data35[,unique(.SD),.SDcols = c("ID","yk")]
hydag.sy <- data05[,unique(.SD),.SDcols = c("ID","yk")]

setkey(hykval.sy,ID,yk); setkey(hydag.sy,ID,yk)
```

```{r}
# station-years in hykvalp-icecorr *not* in hydag:
hykval.sy[, in.hydag := FALSE][hydag.sy, in.hydag := TRUE]
print(hykval.sy[in.hydag==FALSE], nrows = 5)

# station-years in hydag *not* in hykvalp-icecorr:
hydag.sy[, in.hykval := FALSE][hykval.sy, in.hykval := TRUE]
print(hydag.sy[in.hykval==FALSE], nrows = 5)
```

There are 70 years where we have data in HYKVALP-ICECORR but not in HYDAG. For the remaining 14,156 unique station - year combinations where we have data in both HYKVALP-ICECORR and HYDAG, we perform an annual maximum check:

Calculate the annual maxima using HYDAG. Check if HYKVALP-ICECORR contains an observation within +/- 48 hours of the date the annual maximum from HYDAG was observed. If there is no HYKVALP-ICECORR observation within +/- 48 hours of the needed point, discard the year.

We can look at some of the years we discard:

```{r,echo=F, fig.dim=c(10,4), warning=F}
ggplot(lookAMgap.hykval[.(these.AMgap)]) +
  geom_line(aes(date,cumecs.x,color="hykval"),size=1.1) +
  geom_line(data=lookAMgap.hydag[.(these.AMgap)],
            aes(date,cumecs,color="hydag")) +
  labs(y = "m3/s") +
  facet_wrap(ID~yk,scales="free", nrow=1) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),legend.position = "bottom",legend.title = element_blank())
```


### 3. Enforce minimum record length



